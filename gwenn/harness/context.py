"""
Context Manager — Gwenn's Attention Window.

Claude has a finite context window (200K tokens, 1M in beta). This module
manages that window like a precious resource — ensuring the most important
information is always present while gracefully handling overflow.

The context manager handles:
1. TOKEN ESTIMATION: Approximating how many tokens a message history uses
2. COMPACTION: Summarizing older messages to free space when needed
3. PRIORITY-BASED PRUNING: Deciding what to keep and what to compress
4. SYSTEM PROMPT ASSEMBLY: Building the full system prompt from all components

The key principle: the agent should NEVER fail because it ran out of context.
Instead, older information is gracefully compressed, preserving the most
important content while freeing space for new reasoning.
"""

from __future__ import annotations

from typing import Any

import structlog

from gwenn.config import ContextConfig

logger = structlog.get_logger(__name__)


class ContextManager:
    """
    Manages the context window to prevent overflow and optimize content.

    The context window is Gwenn's attention span — everything she can "see"
    at once when thinking. Managing it well is the difference between an
    agent that gracefully handles long conversations and one that crashes
    or loses track of what's happening.
    """

    def __init__(self, config: ContextConfig):
        self._config = config
        self._compaction_count = 0

        logger.info(
            "context_manager.initialized",
            limit=config.context_limit,
            trigger=config.compaction_trigger,
        )

    def estimate_tokens(self, text: str) -> int:
        """
        Estimate token count from text length.

        Uses the rough heuristic of ~4 characters per token. This is
        approximate but sufficient for context management decisions.
        A production system would use tiktoken for exact counts.
        """
        return int(len(text) / self._config.chars_per_token)

    def estimate_message_tokens(self, messages: list[dict[str, Any]]) -> int:
        """Estimate total tokens across all messages."""
        total = 0
        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, str):
                total += self.estimate_tokens(content)
            elif isinstance(content, list):
                # Handle structured content blocks
                for block in content:
                    if isinstance(block, dict):
                        text = block.get("text", "") or block.get("content", "")
                        total += self.estimate_tokens(str(text))
                    else:
                        total += self.estimate_tokens(str(block))
            total += 10  # Overhead per message for role, formatting
        return total

    def needs_compaction(
        self,
        system_prompt: str,
        messages: list[dict[str, Any]],
    ) -> bool:
        """
        Check if the context is approaching the compaction trigger threshold.

        Compaction is triggered when total estimated tokens exceed
        (context_limit * compaction_trigger). This leaves headroom for
        the next response and tool results.
        """
        system_tokens = self.estimate_tokens(system_prompt)
        message_tokens = self.estimate_message_tokens(messages)
        total = system_tokens + message_tokens
        threshold = int(self._config.context_limit * self._config.compaction_trigger)

        if total > threshold:
            logger.info(
                "context_manager.compaction_needed",
                estimated_tokens=total,
                threshold=threshold,
                utilization=round(total / self._config.context_limit, 2),
            )
            return True
        return False

    async def compact(
        self,
        engine: Any,  # CognitiveEngine — avoided circular import
        system_prompt: str,
        messages: list[dict[str, Any]],
    ) -> list[dict[str, Any]]:
        """
        Compact the message history by summarizing older messages.

        The strategy:
        1. Keep the most recent N messages intact (they're most relevant)
        2. Summarize everything before them into a single summary message
        3. Return the compact message history

        The summary is generated by the cognitive engine itself, ensuring
        it captures what the agent considers most important.
        """
        self._compaction_count += 1

        if len(messages) <= 4:
            # Too few messages to compact meaningfully
            return messages

        # Keep the last 4 messages intact (most recent context)
        keep_recent = 4
        old_messages = messages[:-keep_recent]
        recent_messages = messages[-keep_recent:]

        # Ask the engine to summarize the older messages
        compaction_prompt = (
            "Summarize the conversation so far, preserving:\n"
            "1. Key decisions made and their reasoning\n"
            "2. Important facts learned about the user\n"
            "3. Emotional tone and relationship dynamics\n"
            "4. Any unresolved tasks or questions\n"
            "5. Your own internal state changes\n\n"
            "Be thorough but concise. This summary will replace the full history."
        )
        try:
            summary_response = await engine.compact(
                system_prompt, old_messages, compaction_prompt
            )
            summary_text = engine.extract_text(summary_response)
        except Exception as e:
            logger.error("context_manager.compaction_failed", error=str(e))
            # Fallback: just keep recent messages with a note
            summary_text = (
                "[Earlier conversation was summarized due to context limits. "
                f"Original messages: {len(old_messages)}]"
            )

        # Build compacted history
        compacted = [
            {
                "role": "user",
                "content": (
                    f"[CONVERSATION SUMMARY — {len(old_messages)} messages compacted]\n\n"
                    f"{summary_text}"
                ),
            },
            {
                "role": "assistant",
                "content": "I've reviewed the conversation summary and I'm ready to continue.",
            },
        ] + recent_messages

        old_tokens = self.estimate_message_tokens(old_messages)
        new_tokens = self.estimate_message_tokens(compacted[:2])  # Just the summary

        logger.info(
            "context_manager.compacted",
            old_messages=len(old_messages),
            old_tokens=old_tokens,
            summary_tokens=new_tokens,
            compression_ratio=round(new_tokens / max(1, old_tokens), 2),
            total_compactions=self._compaction_count,
        )

        return compacted

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "compaction_count": self._compaction_count,
            "context_limit": self._config.context_limit,
            "compaction_trigger": self._config.compaction_trigger,
        }
