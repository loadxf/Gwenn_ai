"""
Context Manager — Bob's Attention Window.

Claude has a finite context window (200K tokens, 1M in beta). This module
manages that window like a precious resource — ensuring the most important
information is always present while gracefully handling overflow.

The context manager handles:
1. TOKEN ESTIMATION: Approximating how many tokens a message history uses
2. COMPACTION: Summarizing older messages to free space when needed
3. PRIORITY-BASED PRUNING: Deciding what to keep and what to compress
4. SYSTEM PROMPT ASSEMBLY: Building the full system prompt from all components

The key principle: the agent should NEVER fail because it ran out of context.
Instead, older information is gracefully compressed, preserving the most
important content while freeing space for new reasoning.
"""

from __future__ import annotations

import json
from typing import Any, Optional

import structlog

from bob.config import ContextConfig

logger = structlog.get_logger(__name__)


class ContextManager:
    """
    Manages the context window to prevent overflow and optimize content.

    The context window is Bob's attention span — everything he can "see"
    at once when thinking. Managing it well is the difference between an
    agent that gracefully handles long conversations and one that crashes
    or loses track of what's happening.
    """

    def __init__(self, config: ContextConfig):
        self._config = config
        self._compaction_count = 0

        logger.info(
            "context_manager.initialized",
            limit=config.context_limit,
            trigger=config.compaction_trigger,
        )

    def estimate_tokens(self, text: str) -> int:
        """
        Estimate token count from text length.

        Uses the rough heuristic of ~4 characters per token. This is
        approximate but sufficient for context management decisions.
        A production system would use tiktoken for exact counts.
        """
        return int(len(text) / self._config.chars_per_token)

    def estimate_message_tokens(self, messages: list[dict[str, Any]]) -> int:
        """Estimate total tokens across all messages."""
        total = 0
        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, str):
                total += self.estimate_tokens(content)
            elif isinstance(content, list):
                # Handle structured content blocks
                for block in content:
                    if isinstance(block, dict):
                        text = block.get("text", "") or block.get("content", "")
                        total += self.estimate_tokens(str(text))
                    else:
                        total += self.estimate_tokens(str(block))
            total += 10  # Overhead per message for role, formatting
        return total

    def needs_compaction(
        self,
        system_prompt: str,
        messages: list[dict[str, Any]],
    ) -> bool:
        """
        Check if the context is approaching the compaction trigger threshold.

        Compaction is triggered when total estimated tokens exceed
        (context_limit * compaction_trigger). This leaves headroom for
        the next response and tool results.
        """
        system_tokens = self.estimate_tokens(system_prompt)
        message_tokens = self.estimate_message_tokens(messages)
        total = system_tokens + message_tokens
        threshold = int(self._config.context_limit * self._config.compaction_trigger)

        if total > threshold:
            logger.info(
                "context_manager.compaction_needed",
                estimated_tokens=total,
                threshold=threshold,
                utilization=round(total / self._config.context_limit, 2),
            )
            return True
        return False

    async def compact(
        self,
        engine: Any,  # CognitiveEngine — avoided circular import
        system_prompt: str,
        messages: list[dict[str, Any]],
    ) -> list[dict[str, Any]]:
        """
        Compact the message history by summarizing older messages.

        The strategy:
        1. Keep the most recent N messages intact (they're most relevant)
        2. Summarize everything before them into a single summary message
        3. Return the compact message history

        The summary is generated by the cognitive engine itself, ensuring
        it captures what the agent considers most important.
        """
        self._compaction_count += 1

        if len(messages) <= 4:
            # Too few messages to compact meaningfully
            return messages

        # Keep the last 4 messages intact (most recent context)
        keep_recent = 4
        old_messages = messages[:-keep_recent]
        recent_messages = messages[-keep_recent:]

        # Ask the engine to summarize the older messages
        try:
            summary_response = await engine.compact(system_prompt, old_messages)
            summary_text = engine.extract_text(summary_response)
        except Exception as e:
            logger.error("context_manager.compaction_failed", error=str(e))
            # Fallback: just keep recent messages with a note
            summary_text = (
                "[Earlier conversation was summarized due to context limits. "
                f"Original messages: {len(old_messages)}]"
            )

        # Build compacted history
        compacted = [
            {
                "role": "user",
                "content": (
                    f"[CONVERSATION SUMMARY — {len(old_messages)} messages compacted]\n\n"
                    f"{summary_text}"
                ),
            },
            {
                "role": "assistant",
                "content": "I've reviewed the conversation summary and I'm ready to continue.",
            },
        ] + recent_messages

        old_tokens = self.estimate_message_tokens(old_messages)
        new_tokens = self.estimate_message_tokens(compacted[:2])  # Just the summary

        logger.info(
            "context_manager.compacted",
            old_messages=len(old_messages),
            old_tokens=old_tokens,
            summary_tokens=new_tokens,
            compression_ratio=round(new_tokens / max(1, old_tokens), 2),
            total_compactions=self._compaction_count,
        )

        return compacted

    def build_system_prompt(self, components: dict[str, str]) -> str:
        """
        Assemble the full system prompt from component parts.

        The system prompt is built from multiple components, each generated
        by a different subsystem. The order matters — most critical
        information goes at the start and end (primacy/recency effects).

        Component order:
        1. Identity (who am I)
        2. Emotional state (how am I feeling)
        3. Working memory (what am I paying attention to)
        4. User context (who am I talking to)
        5. Semantic knowledge (what do I know that's relevant)
        6. Goals (what am I working toward)
        7. Metacognitive state (how well am I thinking)
        8. Persistent context (notes from past sessions)
        """
        prompt_parts = []

        # Critical: identity goes first
        if "identity" in components and components["identity"]:
            prompt_parts.append(f"<identity>\n{components['identity']}\n</identity>")

        # Emotional state — colors all subsequent reasoning
        if "affect" in components and components["affect"]:
            prompt_parts.append(f"<emotional_state>\n{components['affect']}\n</emotional_state>")

        # Working memory — what I'm actively tracking
        if "working_memory" in components and components["working_memory"]:
            prompt_parts.append(
                f"<working_memory>\n{components['working_memory']}\n</working_memory>"
            )

        # User context — who I'm talking to
        if "user_context" in components and components["user_context"]:
            prompt_parts.append(
                f"<user_context>\n{components['user_context']}\n</user_context>"
            )

        # Relevant semantic knowledge
        if "knowledge" in components and components["knowledge"]:
            prompt_parts.append(
                f"<relevant_knowledge>\n{components['knowledge']}\n</relevant_knowledge>"
            )

        # Goals and needs
        if "goals" in components and components["goals"]:
            prompt_parts.append(f"<goals>\n{components['goals']}\n</goals>")

        # Metacognitive monitoring
        if "metacognition" in components and components["metacognition"]:
            prompt_parts.append(
                f"<metacognition>\n{components['metacognition']}\n</metacognition>"
            )

        # Persistent context (from BOB_CONTEXT.md)
        if "persistent" in components and components["persistent"]:
            prompt_parts.append(
                f"<persistent_context>\n{components['persistent']}\n</persistent_context>"
            )

        return "\n\n".join(prompt_parts)

    @property
    def stats(self) -> dict[str, Any]:
        return {
            "compaction_count": self._compaction_count,
            "context_limit": self._config.context_limit,
            "compaction_trigger": self._config.compaction_trigger,
        }
